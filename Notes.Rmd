---
title: "Заметки"
author: "Илья Мальцев"
date: '21 февраля 2017 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Введение

### Ключевые вопросы

1. Является ли выборка адекватной и достаточной для получения выводов относительно популяции?
2. Существуют ли известные и наблюдаемые, известные и ненаблюдаемые, неизвестные и ненаблюдаемые переменные, которые могут существенно влиять на выводы?
3. Присутствует ли систематическая ошибка, связанная с отсутствием данных или методикой исследования?
4. Что можно сказать о случайности в данных, как она используется? Случайность здесь - или явная в виде случайной выборки, или неявная в виде наложения многих сложных и неизвестных процессов.
5. Пытаемся ли мы определить или оценить механистическую модель, лежащую в основе изучаемого явления.

Статистический вывод требует следования набору предположений, средств и последующего размышления о том, как получить заключения на основе данных.

### Цели

1. Оценить некую популяционную величину и дать количественное значение неопределённости оценки (какова доля людей, которые проголосуют за кандидата?).
2. Determine whether a population quantity is a benchmark value (“is the treatment effective?”).
3. Определить механистическую связь между величинами, измеренными с погрешностями, шумом (каков наклон для закона Гука?).
4. Выявить влияние некоторого фактора (если уменьшить уровень загрязнённости, уменьшится ли частота астматических приступов?).
5. Определить вероятность некоторого явления.

### Средства

1. Рандомизация (Randomization): предназначена для балансировки, исключения влияния ненаблюдаемых переменных, которые могут затруднить получение заключений.
2. Случайные выборки (Random sampling): для получения данных, которые представляют интересующую популяцию.
3. Модели отбора проб (Sampling models): для создания модели процесса выборки. Наиболее часто используется "iid" (independent, identically distributed).
4. Проверка гипотезы (Hypothesis testing): для принятия решения при наличии неопределённости.
5. Доверительные интервалы (Confidence intervals): для количественного выражения неопределённости оценки.
6. Вероятностные модели (Probability models): для формальной связи между данными и исследуемой популяцией.
7. Разработка исследования (Study design): процесс разработки эксперимента для уменьшения ошибок, ограничений, изменчивости.
8. Nonparametric bootstrapping: процесс использования данных для получения выводов с минимальными предположениями о вероятностной модели.
9. Тестирование на перестановках, рандомизации, заменах (Permutation, randomization and exchangeability testing): процесс использования перестановок для получения выводов. 

### Стили

1. Частотная вероятность (Frequency probability): доля событий, произошедших в независимых, одинаково распределённых повторениях, наблюдаемых в большом количестве.
2. Frequency style inference: использует частотную интерпретацию вероятности для контроля уровня ошибки. Отвечает на вопросы: что можно сказать по данным в долгосрочной перспективе, фиксируя определённый уровень ошибки?
3. Байесова вероятность: степень уверенности в истинности суждения.
4. Bayesian style inference: отвечает на вопрос, имея субъективное отношение и объективные данных, следует ли доверять?

## Вероятность

### Аксиоматика Колмогорова

Пусть $\Omega$ множество элементов $\omega$, которые называются элементарными событиями, а $\mathbb{F}$ - множество подмножеств $\Omega$, называемых случайными событиями, а $\Omega$ - пространством элементарных событий.

1. Алгебра событий: $\mathbb{F}$ является алгеброй событий.
2. Существование вероятности событий: каждому событию из $\mathbb{F}$ поставлено в соответствие неотрицательное целое число $\mathbb{P}(x)$, которое называется вероятностью события $x$.
3. Нормировка вероятности: $\mathbb{P}(x)=1$
4. Аддитивность вероятности: если события $x$ и $y$ не пересекаются, то $$\mathbb{P}(x+y)=\mathbb{P}(x)+\mathbb{P}(y)$$

Для случаев с бесконечным числом элементарных событий требуется также аксиома непрерывности:

1. Для убывающей последовательности $$x_1 \supseteq x_2 \supseteq x_n \supseteq ...$$ событий из $\mathbb{F}$ такой, что $$\bigcap_n x_n = \varnothing,$$ имеет место равенство $$\lim_{n\rightarrow\infty} \mathbb{P}(x) = 0.$$

### Функция вероятности

Функция вероятности дискретной величины означает вероятность того, что случайная величина примет заданное значение.

1. $\mathbb{P}(x_i) \ge 0, \forall i$
2. $\sum_{i=1}^\infty \mathbb{P}(x_i) = 1$

### Квантили

Квантиль - значение, которое заданная случайная величина не превышает с фиксированной вероятностью.

Пусть $(\Omega, \mathbb{F}, \mathbb{P})$ - вероятностное пространство, а $\mathbb{P}^X$ - вероятностная мера, задающая распределение некоторой случайной величины $X$. Пусть фиксировано $\alpha \in (0, 1)$, тогда $\alpha$-квантилью (или квантилью уровня $\alpha$) распределения $\mathbb{P}^X$ называется число $x_\alpha$, такое что 
$$\mathbb{P}(X \lt x_\alpha) \le \alpha$$
$$\mathbb{P}(X \ge x_\alpha) \le 1 - \alpha$$

Если распределение непрерывно, то $\alpha$-квантиль однозначно задаётся уравнением $$F_X(x_\alpha) = \alpha,$$
где $F_X$ - функция распределения $\mathbb{P}_X$.

## Условная вероятность

Условная вероятность - вероятность наступления одного события при условии, что другое событие уже произошло.

Пусть $(\Omega, \mathbb{F}, \mathbb{P})$ - вероятностное пространство. Пусть $A, B \in \mathbb{F}$ два случайных события, причём $\mathbb{P}(B) \gt 0$. Тогда условной вероятностью события $A$ при условии события $B$ называется
$$\mathbb{P}(A \mid B)=\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},$$
где $\mathbb{P}(A \cap B)$ - вероятность наступления обоих событий сразу.

### Формула Байеса

$P(A \mid B)=\frac{P(B \mid A)P(A)}{P(B)}$, где:

$P(A)$ - априорная вероятность гипотезы $A$;

$P(A \mid B)$ - вероятность гипотезы $A$ при наступлении события $B$ (апостериорная вероятность);

$P(B \mid A)$ - вероятность наступления события $B$ при истинности гипотезы $A$;

$P(B)$ - полная вероятность наступления события $B$.

В задачах или статистических приложениях $P(B)$ обычно вычисляется по формуле полной вероятности события, зависящего от нескольких несовместных гипотез, имеющих суммарную вероятность 1: $$P(B)=\sum^N_{i=1}P(A_i)P(B \mid A_i,$$
где вероятности под знаком суммы известны или допускают экспериментальную оценку.

Формула Байеса позволяет "переставить причину и следствие": по известному факту события вычислить вероятность того, что оно было вызвано этой причиной.

События, отражающие действие "причин", в данном случае называют "гипотезами", так как они - предполагаемые события, повлекшие данное. Безусловную вероятность справедливости гипотезы называют _априорной_ (насколько вероятна причина вообще), а условную - с учётом факта произошедшего события - _апостериорной_ (насколько вероятна причина оказалась с учётом данных о событии).

### Диагностический тест

_Чувствительность_ - это вероятность того, что тест положительный, если субъект действительно болен $P(+ \mid D)$.

_Специфичность_ - это вероятность того, что тест отрицательный, если субъект не болен $P(- \mid D^c)$.

_Прогностическая ценность положительного результата_ (positive predictive value, PPV) - вероятность, что субъект болен, если тест оказался положительным, $P(D \mid +)$.

_Прогностическая ценность отрицательного результата_ (negative predictive value, NPV) - вероятность, что субъект здоров, если тест оказался отрицательным, $P(D^с \mid -)$.

_Распространённость болезни_ - вероятность болезни во всей популяции, $P(D)$.

_Диагностическое отношение правдоподобия положительного теста_ (diagnostic likelihood ratio of a positive test): $$DLR_+=\frac{P(+\mid D)}{P(+\mid D_c)}=\frac{sensitivity}{1-specificity}$$

_Диагностическое отношение правдоподобия отрицательного теста_ (diagnostic likelihood ratio of a negative test): $$DLR_-=\frac{P(-\mid D)}{P(-\mid D_c)}=\frac{1-sensitivity}{specificity}$$

$DLR_+$ выражает увеличение шансов иметь болезнь после получения положительного результата теста по отношению к исходным шансам.

$DLR_-$ выражает уменьшение шансов иметь болезнь после получения отрицательного результата теста по отношению к исходным шансам.

### Независимые, одинаково распределённые случайные переменные (IID random variables)

Случайные переменные называются независимыми и одинаково распределёнными, если они независимы и получены на одной и той же популяции.

## Ожидаемые значения

### Математическое ожидание:
$$E[X]=\sum_x xp(x)$$

Математическое ожидание выборки:
$$\bar{X}=\sum_{i=1}^n x_i p(x_i),$$
где $p(x_i)=\frac{1}{n}$.

Математическое ожидание выборки является несмещённой оценкой математического ожидания.

Математическое ожидание является центром масс плотности распределения.

### Дисперсия

Дисперсия - мера разброса данных.

$$Var(X)=E[(X-\mu)^2]=E[X^2]-E[X]^2$$

Дисперсия - это математическое ожидание возведённого в квадрат отклонения от среднего значения. Стандартное отклонение - корень из дисперсии.

Дисперсия выборки:
$$S^2=\frac{\sum_{i=1} (X_i-\bar{X})^2}{n-1}$$

### Стандартная ошибка

Вариация среднего значения выборки:
$$Var(\bar{X})=\frac{\sigma^2}{n},$$ 
где $\sigma^2$ - вариация популяции.

Квадратный корень из вариации среднего значения выборки называется стандартной ошибкой.

## Распределения

### Распределение Бернулли

Распределение Бернулли возникает в результате бинарного исхода. Случайная переменная принимает значения 1 и 0 с вероятностями p и (1-з) соответственно. Функция вероятности $P(X=x)=p^x(1-p)^{1-x}$.

$$\mu=\sigma=p(1-p)$$

### Биномиальное распределение

Биномиальная случайная переменная получается как сумма независимых одинаково распределённых бинарных переменных. Если переменная Бернулли - результат броска монеты, то биномиальная переменная - полное число "орлов".

$$P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix} p^x (1-p)^{n-x}$$

$$\begin{pmatrix} n \\ x \end{pmatrix}=\frac{n!}{x!(n-x)!}$$

### Нормальное распределение

1. Приблизительно 68%, 95%, 99% площади под плотностью вероятности лежат внутри 1, 2, 3 стандартных отклонения от среднего.
2. 1.28, 1.645, 1.96, 2.33 соответствуют 90, 97, 97.5, 99-му перцентилям.

### Распределение Пуассона

$$P(X=x; \lambda)=\frac{\lambda^x e^{-\lambda}}{x!}$$

$$\mu=\sigma=\lambda$$

$$x \in [0,\infty)$$

Биномиальное распределение переходит в распределение Пуассона, если n - большое, а p - маленькое. $\lambda=np$.

## Центральная предельная теорема

Распределение усреднений независимых одинаково распределённых переменных становится нормальным с ростом величины выборки.

## Доверительные интервалы

Доверительный интервал является способом количественного выражения неопределённости оценки. Ширина интервала характеризует случайность, препятствующую получению точной оценки.

### Схема построения с использованием центральной предельной теоремы

Пусть $\bar{X}$ - приблизительно нормальная величина со средним $\mu$ и стандартным отклонением $\sigma/\sqrt{n}$.

Тогда значения $\bar{X}>\mu+2\sigma/\sqrt{n}$ составляют только 2.5% (2 здесь указано как два стандартных отклонения). $\bar{X}<\mu-2\sigma/\sqrt{n}$ также составляют около 2.5%. Поэтому суммарная вероятность того, что $\bar{X}$ больше $\mu+2\sigma/\sqrt{n}$ и что $\bar{X}$ меньше $\mu-2\sigma/\sqrt{n}$ составляет 5%. Вероятность, что среднее значение лежит внутри этого интервала составляет 95%. Величина $$\bar{X}\pm 2\sigma/\sqrt{n}$$ называется 95%-ым интервалом для $\mu$. 95% означает, что если провести множество испытаний с размером выборки $n$, то 95% интервалов будут содержать $\mu$.

### Доверительный интервал для биномиального распределения

Пусть $X_i \in {0,1}$ с вероятностью успеха $p$. Интервал принимает вид $\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}$. Заменяя $p$ на $\hat{p}$, получаем доверительный интервал Вальда для $p$. Здесь $\hat{p}$ - вероятность, полученная по выборке. $max(p(1-p))=1/4$, рассмотрим 95% интервал, тогда $z=2$:
$$\hat{p} \pm \frac{1}{\sqrt{n}}$$ - быстрая оценка доверительного интервала.

Если проводить множество испытаний, то интервал Вальда при $p \neq 0.5$ даёт меньшую вероятность попадания среднего значения в него, чем 95%. Это связано с тем, что для ограниченного размера выборки распределение усреднений только приблизительно соответствует нормальному.

Покрытие доверительным интервалом появляющихся в результате экспериментов средних значений может быть существенно улучшено за счёт использования Agresti-Coull интервала. Наиболее простой вариант его построения заключается в добавлении двух успехов и двух неуспехов.

### Доверительный интервал для распределения Пуассона

$$\hat{\lambda} \pm z_{1-\alpha/2}\sqrt{\frac{\hat{\lambda}}{t}}$$

## T-доверительный интервал

Основывается на распределении Стьюдента (Госсета). Применимо для выборок малого размера. При выборе между доверительным интервалом нормального распределения и доверительным интервалом распределения Стьюдента всегда стоит выбрать второй.

Распределение Стьюдента зависит от одного параметра - "степень свободы". Оно предполагает, что данные подчиняются нормальному распределению, независимы и одинаково распределены. Величина

$$\frac{\bar{X}-\mu}{S/\sqrt{n}}$$

подчиняется распределению Стьюдента с $n-1$ степенями свободы.

Доверительный интервал: $\bar{X} \pm t_{n-1}S/\sqrt{n}$.

1. T-интервал предполагает, что данные независимо и одинаково распределены согласно нормальному распределению.
2. Хорошо работает, если распределение данных симметрично и имеет форму кургана.
3. Парные наблюдения часто анализируются с использованием T-интервала для разностей.
4. Для большого количества степеней свободы, T-квантили соответствуют квантилям нормального распределения.
5. Для скошенных распределений можно пользоваться логарифмической шкалой, медианой и т. д.
6. Для дискретных данных следует выбирать другие интервалы.

## Проверка гипотез

Классическая проверка гипотез заключается в выборе между двумя гипотезами. Первая представляет статус-кво. Это предположение по умолчанию. Альтернативная гипотеза - это выдвинутая гипотеза, которая нуждается в проверке.

### Типы ошибок при проверке гипотез

|Истина|Решение|Результат|
|---|---|---|
|$H_0$|$H_0$|Верное решение о справедливости нулевой гипотезы|
|$H_0$|$H_a$|Ошибка первого типа|
|$H_a$|$H_a$|Верное решение об ошибочности нулевой гипотезы|
|$H_a$|$H_0$|Ошибка второго типа|

Уменьшая строгость при принятии решения об отклонении нулевой гипотезы, мы увеличиваем шанс получить ошибку первого типа, но уменьшаем шанс ошибки второго типа.

Увеличивая строгость при принятии решения об отклонении нулевой гипотезы, мы уменьшаем шанс получить ошибку первого типа, но увеличиваем шанс на ошибку второго типа.

### Этапы проверки

1. Формулировка основной гипотезы и альтернативной гипотезы.

2. Задание уровня значимости $\alpha$. Он равен вероятности допустить ошибку первого рода.

3. Расчёт статистики $\phi$ критерия такой, что:
* её величина зависит от исходной выборки;
* по её значению можно сделать выбор об истинности нулевой гипотезы;
* статистика $\phi$, как функция случайной величины $\mathbf{X}$, также является случайной величиной и подчиняется какому-то закону распределения.

4. Построение критической области. Из области значений $\phi$  выделяется подмножество $\mathbb{C}$ таких значений, по которым можно судить о существенных расхождениях с предположением. Его размер выбирается таким образом, чтобы выполнялось равенство $P(\phi \in \mathbb{C})=\alpha$ . Это множество $\mathbb{C}$ и называется критической областью.

5. Вывод об истинности гипотезы. Наблюдаемые значения выборки подставляются в статистику $\phi$ и по попаданию (или непопаданию) в критическую область $\mathbb{C}$ выносится решение об отвержении (или принятии) выдвинутой гипотезы $H_{0}$.

### Связь с доверительными интервалами

Пусть $H_{0}: \mu=\mu_{0}$ и $H_{a}: \mu \neq \mu_{0}$. Множество всех возможных значений, для которых гипотезу $H_{0}$ отклонить не удастся составляет $(1-\alpha)*100%$-доверительный интервал для $\mu$.

И обратно, если $(1-\alpha)100%$-интервал содержит $\mu_{0}$, то гипотеза $H_{0}$ остаётся неопровергнутой.

### Интервал для двух групп

Рассмотрим $H_{0}: \mu=\mu_{0}$. $$\frac{\bar{X_1}-\bar{X_2}-(\mu_{1}-\mu_{2})}{S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$$ - статистика для групп с равными дисперсиями.

$$\frac{\bar{X_1}-\bar{X_2}-(\mu_{1}-\mu_{2})}{\sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}}$$ - статистика для случая разных дисперсий.

## P-значения

P-значение - это вероятность того, что случайная величина с данным распределением (распределением тестовой статистики при нулевой гипотезе) примет значение, не меньшее, чем фактическое значение тестовой статистики.

Вычисление P-значения:

1. Выбор статистики.
2. Выбор распределения для статистики.
3. Вычисление вероятности получить статистику того же или более экстремального значения, чем выбранная статистика, с использованием выбранного распределения.

### Достижимый уровень значимости

Наименьшее значение $\alpha$, при котором нулевая гипотеза ещё отклоняется, называется достижимым уровнем значимости. Математически оно эквивалентно P-значению.

### Мощность

Мощность  - это вероятность отклонения нулевой гипотезы, если она неверна.

$$Power=1-Type 2 error$$

