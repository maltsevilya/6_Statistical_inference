---
title: "Заметки"
author: "Илья Мальцев"
date: '21 февраля 2017 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Введение

### Ключевые вопросы

1. Является ли выборка адекватной и достаточной для получения выводов относительно популяции?
2. Существуют ли известные и наблюдаемые, известные и ненаблюдаемые, неизвестные и ненаблюдаемые переменные, которые могут существенно влиять на выводы?
3. Присутствует ли систематическая ошибка, связанная с отсутствием данных или методикой исследования?
4. Что можно сказать о случайности в данных, как она используется? Случайность здесь - или явная в виде случайной выборки, или неявная в виде наложения многих сложных и неизвестных процессов.
5. Пытаемся ли мы определить или оценить механистическую модель, лежащую в основе изучаемого явления.

Статистический вывод требует следования набору предположений, средств и последующего размышления о том, как получить заключения на основе данных.

### Цели

1. Оценить некую популяционную величину и дать количественное значение неопределённости оценки (какова доля людей, которые проголосуют за кандидата?).
2. Determine whether a population quantity is a benchmark value (“is the treatment effective?”).
3. Определить механистическую связь между величинами, измеренными с погрешностями, шумом (каков наклон для закона Гука?).
4. Выявить влияние некоторого фактора (если уменьшить уровень загрязнённости, уменьшится ли частота астматических приступов?).
5. Определить вероятность некоторого явления.

### Средства

1. Рандомизация (Randomization): предназначена для балансировки, исключения влияния ненаблюдаемых переменных, которые могут затруднить получение заключений.
2. Случайные выборки (Random sampling): для получения данных, которые представляют интересующую популяцию.
3. Модели отбора проб (Sampling models): для создания модели процесса выборки. Наиболее часто используется "iid" (independent, identically distributed).
4. Проверка гипотезы (Hypothesis testing): для принятия решения при наличии неопределённости.
5. Доверительные интервалы (Confidence intervals): для количественного выражения неопределённости оценки.
6. Вероятностные модели (Probability models): для формальной связи между данными и исследуемой популяцией.
7. Разработка исследования (Study design): процесс разработки эксперимента для уменьшения ошибок, ограничений, изменчивости.
8. Nonparametric bootstrapping: процесс использования данных для получения выводов с минимальными предположениями о вероятностной модели.
9. Тестирование на перестановках, рандомизации, заменах (Permutation, randomization and exchangeability testing): процесс использования перестановок для получения выводов. 

### Стили

1. Частотная вероятность (Frequency probability): доля событий, произошедших в независимых, одинаково распределённых повторениях, наблюдаемых в большом количестве.
2. Frequency style inference: использует частотную интерпретацию вероятности для контроля уровня ошибки. Отвечает на вопросы: что можно сказать по данным в долгосрочной перспективе, фиксируя определённый уровень ошибки?
3. Байесова вероятность: степень уверенности в истинности суждения.
4. Bayesian style inference: отвечает на вопрос, имея субъективное отношение и объективные данных, следует ли доверять?

## Вероятность

### Аксиоматика Колмогорова

Пусть $\Omega$ множество элементов $\omega$, которые называются элементарными событиями, а $\mathbb{F}$ - множество подмножеств $\Omega$, называемых случайными событиями, а $\Omega$ - пространством элементарных событий.

1. Алгебра событий: $\mathbb{F}$ является алгеброй событий.
2. Существование вероятности событий: каждому событию из $\mathbb{F}$ поставлено в соответствие неотрицательное целое число $\mathbb{P}(x)$, которое называется вероятностью события $x$.
3. Нормировка вероятности: $\mathbb{P}(x)=1$
4. Аддитивность вероятности: если события $x$ и $y$ не пересекаются, то $$\mathbb{P}(x+y)=\mathbb{P}(x)+\mathbb{P}(y)$$

Для случаев с бесконечным числом элементарных событий требуется также аксиома непрерывности:

1. Для убывающей последовательности $$x_1 \supseteq x_2 \supseteq x_n \supseteq ...$$ событий из $\mathbb{F}$ такой, что $$\bigcap_n x_n = \varnothing,$$ имеет место равенство $$\lim_{n\rightarrow\infty} \mathbb{P}(x) = 0.$$

### Функция вероятности

Функция вероятности дискретной величины означает вероятность того, что случайная величина примет заданное значение.

1. $\mathbb{P}(x_i) \ge 0, \forall i$
2. $\sum_{i=1}^\infty \mathbb{P}(x_i) = 1$

### Квантили

Квантиль - значение, которое заданная случайная величина не превышает с фиксированной вероятностью.

Пусть $(\Omega, \mathbb{F}, \mathbb{P})$ - вероятностное пространство, а $\mathbb{P}^X$ - вероятностная мера, задающая распределение некоторой случайной величины $X$. Пусть фиксировано $\alpha \in (0, 1)$, тогда $\alpha$-квантилью (или квантилью уровня $\alpha$) распределения $\mathbb{P}^X$ называется число $x_\alpha$, такое что 
$$\mathbb{P}(X \lt x_\alpha) \le \alpha$$
$$\mathbb{P}(X \ge x_\alpha) \le 1 - \alpha$$

Если распределение непрерывно, то $\alpha$-квантиль однозначно задаётся уравнением $$F_X(x_\alpha) = \alpha,$$
где $F_X$ - функция распределения $\mathbb{P}_X$.

## Условная вероятность

Условная вероятность - вероятность наступления одного события при условии, что другое событие уже произошло.

Пусть $(\Omega, \mathbb{F}, \mathbb{P})$ - вероятностное пространство. Пусть $A, B \in \mathbb{F}$ два случайных события, причём $\mathbb{P}(B) \gt 0$. Тогда условной вероятностью события $A$ при условии события $B$ называется
$$\mathbb{P}(A \mid B)=\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},$$
где $\mathbb{P}(A \cap B)$ - вероятность наступления обоих событий сразу.

### Формула Байеса

$P(A \mid B)=\frac{P(B \mid A)P(A)}{P(B)}$, где:

$P(A)$ - априорная вероятность гипотезы $A$;

$P(A \mid B)$ - вероятность гипотезы $A$ при наступлении события $B$ (апостериорная вероятность);

$P(B \mid A)$ - вероятность наступления события $B$ при истинности гипотезы $A$;

$P(B)$ - полная вероятность наступления события $B$.

В задачах или статистических приложениях $P(B)$ обычно вычисляется по формуле полной вероятности события, зависящего от нескольких несовместных гипотез, имеющих суммарную вероятность 1: $$P(B)=\sum^N_{i=1}P(A_i)P(B \mid A_i,$$
где вероятности под знаком суммы известны или допускают экспериментальную оценку.

Формула Байеса позволяет "переставить причину и следствие": по известному факту события вычислить вероятность того, что оно было вызвано этой причиной.

События, отражающие действие "причин", в данном случае называют "гипотезами", так как они - предполагаемые события, повлекшие данное. Безусловную вероятность справедливости гипотезы называют _априорной_ (насколько вероятна причина вообще), а условную - с учётом факта произошедшего события - _апостериорной_ (насколько вероятна причина оказалась с учётом данных о событии).

### Диагностический тест

_Чувствительность_ - это вероятность того, что тест положительный, если субъект действительно болен $P(+ \mid D)$.

_Специфичность_ - это вероятность того, что тест отрицательный, если субъект не болен $P(- \mid D^c)$.

_Прогностическая ценность положительного результата_ (positive predictive value, PPV) - вероятность, что субъект болен, если тест оказался положительным, $P(D \mid +)$.

_Прогностическая ценность отрицательного результата_ (negative predictive value, NPV) - вероятность, что субъект здоров, если тест оказался отрицательным, $P(D^с \mid -)$.

_Распространённость болезни_ - вероятность болезни во всей популяции, $P(D)$.

_Диагностическое отношение правдоподобия положительного теста_ (diagnostic likelihood ratio of a positive test): $$DLR_+=\frac{P(+\mid D)}{P(+\mid D_c)}=\frac{sensitivity}{1-specificity}$$

_Диагностическое отношение правдоподобия отрицательного теста_ (diagnostic likelihood ratio of a negative test): $$DLR_-=\frac{P(-\mid D)}{P(-\mid D_c)}=\frac{1-sensitivity}{specificity}$$

$DLR_+$ выражает увеличение шансов иметь болезнь после получения положительного результата теста по отношению к исходным шансам.

$DLR_-$ выражает уменьшение шансов иметь болезнь после получения отрицательного результата теста по отношению к исходным шансам.

### Независимые, одинаково распределённые случайные переменные (IID random variables)

Случайные переменные называются независимыми и одинаково распределёнными, если они независимы и получены на одной и той же популяции.

## Ожидаемые значения

Математическое ожидание:
$$E[X]=\sum_x xp(x)$$
